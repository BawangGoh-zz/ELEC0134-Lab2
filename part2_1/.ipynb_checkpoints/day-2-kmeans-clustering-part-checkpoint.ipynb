{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Day 2: Clustering and Dimensionality Reduction\n",
    "## K-Means & Principal Components Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to implement clustering with and without dimensionality reduction. In particular, you will:\n",
    "\n",
    "* Complete a function `kmeansNewPredict(X, k)` to implement k-means algorithm from scratch for different number of clusters k=2,3,4,5.\n",
    "* Use the function `PCA()` from scikit-learn in order to reduce the dimension of data to d = 2. Plot the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The required libraries for this notebook are pandas, sklearn, copy, numpy, pickle, mpl_toolkits and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "For the clustering tasks we will use the dataset clustering_data.pkl. It consists of 3000 data points. Each data point has 2 features (2D data). The data points come from different distributions (between 2 and 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the pickle file\n",
    "data_file = open('./dataset/clustering_data.pkl','rb')\n",
    "data = pickle.load(data_file, encoding='latin1')\n",
    "data_file.close()\n",
    "X = data['X']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use K-means clustering from a library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first see how k-means can be implemented using already available functions from the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn functions implementation\n",
    "def kmeansPredict(X, k):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(X)\n",
    "    y_kmeans = kmeans.predict(X)\n",
    "    return y_kmeans, kmeans.cluster_centers_\n",
    "\n",
    "# run K-means for different values of k\n",
    "k1 = 2\n",
    "y_predict, centers = kmeansPredict(X,k1)\n",
    "#  plot the clusters \n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_predict, s=50, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n",
    "print(centers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement you own K-means clustering function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are asked to calculate the missing variables in the `kmeansNewPredict(X,k)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Euclidean Distance Caculator\n",
    "def dist(a, b, ax=1):\n",
    "    return np.linalg.norm(a - b, axis=ax)\n",
    "\n",
    "def kmeansNewPredict(X,k):\n",
    "    # Generate X and Y coordinates of random centroids\n",
    "    C_x = np.random.randint(0, np.max(X), size=k)\n",
    "    C_y = np.random.randint(0, np.max(X), size=k)\n",
    "    C = np.array(list(zip(C_x, C_y)), dtype=np.float32)\n",
    "\n",
    "    C_old = np.zeros(C.shape)\n",
    "    clusters = np.zeros(len(X))\n",
    "    # Error: Distance between new centroids and old centroids\n",
    "    error = dist(C, C_old, None)\n",
    "    # Loop will run till the error becomes zero\n",
    "    while error != 0:\n",
    "        # Assigning each value to its closest cluster\n",
    "        for i in range(len(X)):\n",
    "            distances = dist(X[i], C)\n",
    "            #cluster = ...\n",
    "            clusters[i] = cluster\n",
    "        # Storing the old centroid values\n",
    "        C_old = deepcopy(C)\n",
    "        # Finding the new centroids by taking the average value\n",
    "        for i in range(k):\n",
    "            #points = ...\n",
    "            #C[i] = ...\n",
    "        error = dist(C, C_old, None)\n",
    "\n",
    "\n",
    "    return points, C\n",
    "\n",
    "# run K-means for different values of k\n",
    "k2 = 2\n",
    "y_kmeans1, centers1 = kmeansNewPredict(X,k2)\n",
    "\n",
    "#  plot the clusters \n",
    "fig, ax = plt.subplots()\n",
    "for i in range(k2):\n",
    "        y_kmeans1 = np.array([X[j] for j in range(len(X))])\n",
    "        ax.scatter(y_kmeans1[:, 0], y_kmeans1[:, 1], s=50)\n",
    "ax.scatter(centers1[:, 0], centers1[:, 1],  c='black', s=200, alpha=0.5);\n",
    "\n",
    "print(centers1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the iris dataset for PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the iris dataset available from scikit learn library, which includes 4 feature columns (sepal length, sepal width, petal length, and petal width) and 150 data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset=load_iris()\n",
    "\n",
    "#print(dataset.data.shape) # shape of data\n",
    "#print(dataset.target.shape)\n",
    "#print(boston.feature_names)\n",
    "dataset_df=pd.DataFrame(dataset.data,columns=dataset.feature_names) # convert the boston.data to a a dataframe\n",
    "print(dataset_df.head())\n",
    "\n",
    "X_iris=dataset.data\n",
    "y_iris=dataset.target\n",
    "\n",
    "#print(X.shape)\n",
    "#print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# to enable graph interactivity \n",
    "%matplotlib notebook  \n",
    "fig_pca=plt.figure(1, figsize=(4,3))\n",
    "plot=Axes3D(fig_pca, rect=[0,0,.95,1],elev=48, azim=134)\n",
    "\n",
    "\n",
    "for name, label in [('Setosa',0), ('Versicolour',1), ('Virginica',2)]:\n",
    "    plot.text3D(X_iris[y_iris==label,0].mean(),\n",
    "                X_iris[y_iris==label,1].mean()+1.5,\n",
    "                X_iris[y_iris==label,2].mean(),name, \n",
    "                horizontalalignment='center',\n",
    "                bbox=dict(alpha=.5, edgecolor='w',facecolor='w') )\n",
    "\n",
    "\n",
    "\n",
    "y_iris=y_iris.astype(np.float)    \n",
    "\n",
    "\n",
    "\n",
    "plot.scatter(X_iris[:, 0], X_iris[:, 1], X_iris[:, 2], c=y_iris,  cmap=plt.cm.nipy_spectral, edgecolor='k')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale your data\n",
    "\n",
    "Before we apply PCA, the data features (i.e. X) should be scaled (standardized) onto unit scale (mean = 0 and variance = 1). You could do so by using `StandardScaler()` function from scikit-learn.\n",
    "\n",
    "For more information on the importance of feature scaling, please visit: **https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call the function\n",
    "sc = StandardScaler() \n",
    "# Standardizing the features\n",
    "X_scale = sc.fit_transform(X_iris)  \n",
    "#print(X_iris.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project data to 2D using PCA\n",
    "\n",
    "In the following cell you should implement the projection of the original data into 2 dimensions. Then apply kmeans and compare your results with and without PCA.\n",
    "\n",
    "**Note 1:** The original data is 4 dimensional.\n",
    "\n",
    "**Note 2:** The principal components you will create with dimensionality reduction, don't have a particular meaning assigned (usually). They are just the two main dimensions of variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pca = ... #call PCA\n",
    "#X_new = ... # fit\n",
    "print('Information included per principal component (%): '+str(pca.explained_variance_ratio_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_new, centers_new = kmeansPredict(X_new,k)\n",
    "print( centers_new)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
